{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJU2RPpSvlQT"
      },
      "source": [
        "# Residual Netowrk・Wide Residual Network（CIFAR10を用いた物体認識）\n",
        "\n",
        "\n",
        "---\n",
        "## 目的\n",
        "CIFAR10 Datasetを用いて10クラスの物体認識を行う．\n",
        "\n",
        "ここでは畳み込みニューラルネットワークのモデルとして，ResNetおよびWideResNetを用いて実験を行う．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQGfxWYK_4O"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo4jjpmwvle1"
      },
      "source": [
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "### GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`Use CUDA: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n",
        "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCeaCulfvlao"
      },
      "outputs": [],
      "source": [
        "# モジュールのインポート\n",
        "from time import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppjeW5MbysXC"
      },
      "source": [
        "## データセットの読み込みと確認\n",
        "\n",
        "学習データ（CIFAR10データセット）を読み込みます．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_xx-TkVvls6"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=1),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor()])\n",
        "transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=transform_train, download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=transform_test, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj1ja-fGLV8e"
      },
      "source": [
        "## Residual Network (ResNet)\n",
        "Residual Network (ResNet) を定義します．\n",
        "\n",
        "ResNetはBottleneckと呼ばれる構造から構成されています．\n",
        "まず，`BottleNeck(nn.Module)`で，任意の形のBottleNeckを定義できるクラスを作成します．\n",
        "`__init__`関数の引数である，`in_planes`は入力される特徴マップのチャンネル数，`planes`はBottleNeck内の特徴マップのチャンネル数を指定します．\n",
        "\n",
        "また，層を定義する`__init__`内では，`nn.Sequential()`という関数が用いられています．\n",
        "これは，複数の層が格納されたリストを引数として受け取り，これらの層をひとまとめにしたオブジェクト（層）を定義する関数です・\n",
        "下の関数では，畳み込みやBatchNormalizationがリスト内にされています．\n",
        "`nn.Sequential`で定義した層`self.convs`では，実際に演算する際，すなわち`formward()`関数内では，`self.convs(x)`とすることで，リストに格納した演算をその順番通りに処理して返すことができます．\n",
        "\n",
        "上で定義したBottleNeck構造を活用して，ResNet（ここではResNet50）を定義します．\n",
        "`ResNet`クラス内で定義されている`self._make_layer()`は，任意の形（総数）のResidual Block (複数のBottleNeck構造からなる層)を定義します．\n",
        "Residual Blockに入力されるチャンネル数`planes`，BottleNeckの数`num_blocks`，畳み込みのストライド`stride`を指定します．\n",
        "その後，それらの引数に従い，指定した数・パラメータのBottleNeckをリストないに格納します．\n",
        "最後に，上で説明した`nn.Sequential`を用いて一塊の層として定義し，返すことで，任意の数の層を持つresidual blockを定義します．\n",
        "\n",
        "この`_make_layer()`を用いて，`__init__`でResNet全体を定義します．\n",
        "`AdaptiveAvgPooling()`は任意のサイズの特徴マップに対して平均プーリングを適用する層です．\n",
        "引数に`(1, 1)`を指定することで，どのようなサイズの特徴マップが入力された場合でも，1×1の特徴マップになるように平均プーリングを行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNHnp_YczmY3"
      },
      "outputs": [],
      "source": [
        "class BottleNeck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.convs = nn.Sequential(*[nn.Conv2d(in_planes, planes, kernel_size=1, bias=False),\n",
        "                                     nn.BatchNorm2d(planes),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "                                     nn.BatchNorm2d(planes),\n",
        "                                     nn.ReLU(inplace=True),\n",
        "                                     nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False),\n",
        "                                     nn.BatchNorm2d(self.expansion * planes)])\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.convs(x)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, n_class=10, n_blocks=[3, 4, 6, 3]):\n",
        "        super().__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.res1 = self._make_layer(64, n_blocks[0], stride=1)\n",
        "        self.res2 = self._make_layer(128, n_blocks[1], stride=2)\n",
        "        self.res3 = self._make_layer(256, n_blocks[2], stride=2)\n",
        "        self.res4 = self._make_layer(512, n_blocks[3], stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(2048, n_class)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BottleNeck(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * BottleNeck.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.relu(self.bn1(self.conv1(x)))\n",
        "        h = self.res1(h)\n",
        "        h = self.res2(h)\n",
        "        h = self.res3(h)\n",
        "        h = self.res4(h)\n",
        "        h = self.avgpool(h)\n",
        "        h = torch.flatten(h, 1)\n",
        "        h = self.fc(h)\n",
        "        return h\n",
        "        \n",
        "        \n",
        "class ResNet50(ResNet):\n",
        "    def __init__(self, n_class=10):\n",
        "        super(ResNet50, self).__init__(n_class, n_blocks=[3, 4, 6, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUNa9Xe79vAG"
      },
      "source": [
        "### 学習\n",
        "\n",
        "上記で定義したResNetを用いて，ネットワークの学習を行います．\n",
        "学習のプログラムに関しては，前回までのCIFAR10を用いた演習と同様のため，詳細は割愛します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68RE3RTa76-W"
      },
      "outputs": [],
      "source": [
        "# ネットワークモデル・最適化手法の設定\n",
        "model_res = ResNet50(n_class=10)\n",
        "if use_cuda:\n",
        "    model_res.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model_res.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 128\n",
        "epoch_num = 10\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model_res.train()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model_res(image)\n",
        "        loss = criterion(y, label)\n",
        "        \n",
        "        model_res.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "119eIrSmzmw6"
      },
      "source": [
        "### テスト\n",
        "学習したネットワークモデルを用いて評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoYVMRGLzm1I"
      },
      "outputs": [],
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model_res.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            \n",
        "        y = model_res(image)\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / 10000.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47bVCZ3zLV8h"
      },
      "source": [
        "## Wide Residual Network (WideResNet; WRN)\n",
        "\n",
        "次に，Wide Residual Network (WideResNet; WRN) を定義します．\n",
        "\n",
        "WideResNetは，ResNetなどと比べ，ネットワークのフィルタ枚数を増やし（すなわちWideにし），相対的に層を浅くしたネットワークである．\n",
        "\n",
        "WideResNetでは，ResNetと類似したBlock構造から構成されており，まず初めに基本的なBlock構造（`BasicBlock`）を定義します．\n",
        " __init__関数の引数である，in_planesは入力される特徴マップのチャンネル数，planesはBottleNeck内の特徴マップのチャンネル数を指定します．\n",
        "また，この時，演算の順番が異なることに注意してください．\n",
        "\n",
        "次に，WideResNetの一部にあたる，`NetworkBlock`モジュールを作成します．\n",
        "ここでは，前述のnn.Sequential()という関数を活用し，指定した数だけ，BasicBlockを連結したモジュールを作成します．\n",
        "\n",
        "最後に，これらのモジュールを用いて，WideResNetを作成します．\n",
        "引数として，ネットワークの深さの`depth`, チャンネル数を増やす割合を示す`widen_factor`を指定します．\n",
        "`dropRate`はネットワーク内のDropoutの割合を決定するパラメータです．\n",
        "\n",
        "指定した`depth`および`widen_afctor`から，ネットワーク構造を決定し，作成します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7yILVJLLV8h"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    \n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes=100, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = int((depth - 4) / 6)\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmJsdjQZLV8i"
      },
      "source": [
        "### 学習\n",
        "\n",
        "上記で定義したWideResNetを用いて，ネットワークの学習を行います．\n",
        "この時，ネットワークの深さの`depth`とチャンネル数を決定する`wide`を指定し，ネットワークモデルを作成します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc_F69JdLV8i"
      },
      "outputs": [],
      "source": [
        "# ネットワークモデル・最適化手法の設定\n",
        "depth = 16  # e.g.16,22,28,40\n",
        "wide  = 2\n",
        "model_wrn = WideResNet(depth, widen_factor=wide)\n",
        "if use_cuda:\n",
        "    model_wrn.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model_wrn.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 128\n",
        "epoch_num = 10\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model_wrn.train()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model_wrn(image)\n",
        "        loss = criterion(y, label)\n",
        "        \n",
        "        model_wrn.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjEABkHHLV8j"
      },
      "source": [
        "### テスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W__FsLCLV8j"
      },
      "outputs": [],
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model_wrn.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "            \n",
        "        y = model_wrn(image)\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / 10000.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U8wsW37hUUI"
      },
      "source": [
        "## 課題\n",
        "\n",
        "1. 学習の設定を変更し，認識精度の変化を確認しましょう．\n",
        "2. WideResNetのパラメータを変更して精度の変化を確認してみましょう．\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "13_cifar_resnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}